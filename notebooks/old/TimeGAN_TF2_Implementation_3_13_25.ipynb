{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qd9_N6wzSsY9"
   },
   "source": [
    "# TimeGAN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "35KnkzSF5XDc"
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Zwib3BoV5Vrz"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1) Utilities: batch generator, min-max scaling\n",
    "###############################################################################\n",
    "\n",
    "def batch_generator(data, batch_size):\n",
    "    \"\"\"\n",
    "    Randomly samples a subset of data (batch_size)\n",
    "    \"\"\"\n",
    "    idx = np.random.permutation(len(data))[:batch_size]\n",
    "    return data[idx]\n",
    "\n",
    "def min_max_scale(data):\n",
    "    \"\"\"\n",
    "    Scale entire 3D array to [0,1].\n",
    "    Returns scaled_data, min_val, max_val (all float32).\n",
    "    \"\"\"\n",
    "    min_val = np.min(data, axis=(0,1), keepdims=True)\n",
    "    data_ = data - min_val\n",
    "    max_val = np.max(data_, axis=(0,1), keepdims=True)\n",
    "    data_ = data_ / (max_val + 1e-7)\n",
    "    return data_.astype(np.float32), min_val.astype(np.float32), max_val.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vcD-wrDHWVE_"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 2) Data: sine generation, float32\n",
    "###############################################################################\n",
    "\n",
    "def sine_data_generation(\n",
    "    n_samples=10000,        # no. samples\n",
    "    seq_len=1.0,            # total duration in seconds\n",
    "    dim=1,                  # no. channels\n",
    "    sampling_rate=100.0,    # samples per second\n",
    "    base_freq=5.0           # mean frequency in Hz\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate n_samples of random sine waves.\n",
    "    Paraemters:\n",
    "      - seq_len = total duration of waveform (in seconds)\n",
    "      - sampling_rate = how many samples per second (Hz)\n",
    "      - base_freq = frequency in cycles/second (Hz)\n",
    "\n",
    "    The output array has shape:\n",
    "      (n_samples, n_timesteps, dim)\n",
    "    where n_timesteps = seq_len * sampling_rate\n",
    "    \"\"\"\n",
    "\n",
    "    n_timesteps = int(seq_len * sampling_rate)\n",
    "    t = np.linspace(0, seq_len, n_timesteps, endpoint=False)\n",
    "\n",
    "    data = []\n",
    "    for _ in range(n_samples):\n",
    "        seq = []\n",
    "        for _ in range(dim):\n",
    "            f = np.random.normal(loc=base_freq, scale=0.5)\n",
    "            phase = np.random.uniform(0, 2*np.pi)\n",
    "            x = np.sin(2 * np.pi * f * t + phase)\n",
    "            x = (x + 1)*0.5\n",
    "            seq.append(x)\n",
    "        seq = np.transpose(np.array(seq))\n",
    "        data.append(seq)\n",
    "\n",
    "    data = np.array(data, dtype=np.float32)  # shape is (n_samples, n_timesteps, dim)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lljEI7-Y5RB3"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 3) Sub-networks\n",
    "###############################################################################\n",
    "\n",
    "# Embedder\n",
    "class Embedder(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim=24, num_layers=3, rnn_type='gru'):\n",
    "        super().__init__()\n",
    "        self.rnns = []\n",
    "        for _ in range(num_layers):\n",
    "            if rnn_type=='gru':\n",
    "                self.rnns.append(layers.GRU(hidden_dim, return_sequences=True, activation='tanh'))\n",
    "            else:\n",
    "                self.rnns.append(layers.LSTM(hidden_dim, return_sequences=True, activation='tanh'))\n",
    "        self.fc = layers.Dense(hidden_dim, activation='sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        h = x\n",
    "        for rnn in self.rnns:\n",
    "            h = rnn(h)\n",
    "        return self.fc(h)\n",
    "\n",
    "# Recovery\n",
    "class Recovery(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim=24, out_dim=5, num_layers=3, rnn_type='gru'):\n",
    "        super().__init__()\n",
    "        self.rnns = []\n",
    "        for _ in range(num_layers):\n",
    "            if rnn_type=='gru':\n",
    "                self.rnns.append(layers.GRU(hidden_dim, return_sequences=True, activation='tanh'))\n",
    "            else:\n",
    "                self.rnns.append(layers.LSTM(hidden_dim, return_sequences=True, activation='tanh'))\n",
    "        self.fc = layers.Dense(out_dim, activation='sigmoid')\n",
    "\n",
    "    def call(self, h):\n",
    "        x_ = h\n",
    "        for rnn in self.rnns:\n",
    "            x_ = rnn(x_)\n",
    "        return self.fc(x_)\n",
    "\n",
    "# Generator\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim=24, num_layers=3, rnn_type='gru'):\n",
    "        super().__init__()\n",
    "        self.rnns = []\n",
    "        for _ in range(num_layers):\n",
    "            if rnn_type=='gru':\n",
    "                self.rnns.append(layers.GRU(hidden_dim, return_sequences=True, activation='tanh'))\n",
    "            else:\n",
    "                self.rnns.append(layers.LSTM(hidden_dim, return_sequences=True, activation='tanh'))\n",
    "        self.fc = layers.Dense(hidden_dim, activation='sigmoid')\n",
    "\n",
    "    def call(self, z):\n",
    "        e = z\n",
    "        for rnn in self.rnns:\n",
    "            e = rnn(e)\n",
    "        return self.fc(e)\n",
    "\n",
    "# Supervisor\n",
    "class Supervisor(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim=24, num_layers=3, rnn_type='gru'):\n",
    "        super().__init__()\n",
    "        self.rnns = []\n",
    "        for _ in range(num_layers-1):\n",
    "            if rnn_type=='gru':\n",
    "                self.rnns.append(layers.GRU(hidden_dim, return_sequences=True, activation='tanh'))\n",
    "            else:\n",
    "                self.rnns.append(layers.LSTM(hidden_dim, return_sequences=True, activation='tanh'))\n",
    "        self.fc = layers.Dense(hidden_dim, activation='sigmoid')\n",
    "\n",
    "    def call(self, h):\n",
    "        s = h\n",
    "        for rnn in self.rnns:\n",
    "            s = rnn(s)\n",
    "        return self.fc(s)\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim=24, num_layers=3, rnn_type='gru'):\n",
    "        super().__init__()\n",
    "        self.rnns = []\n",
    "        for _ in range(num_layers):\n",
    "            if rnn_type=='gru':\n",
    "                self.rnns.append(layers.GRU(hidden_dim, return_sequences=True, activation='tanh'))\n",
    "            else:\n",
    "                self.rnns.append(layers.LSTM(hidden_dim, return_sequences=True, activation='tanh'))\n",
    "        self.fc = layers.Dense(1, activation=None)  # logits\n",
    "\n",
    "    def call(self, h):\n",
    "        d = h\n",
    "        for rnn in self.rnns:\n",
    "            d = rnn(d)\n",
    "        return self.fc(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Hbcu0pEl5Q6O"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 4) TimeGAN class: we explicitly build each sub-network with dummy data\n",
    "###############################################################################\n",
    "\n",
    "class TimeGAN(tf.keras.Model):\n",
    "    def __init__(self, seq_len, dim, hidden_dim=24, num_layers=3, module='gru', gamma=1.0):\n",
    "        \"\"\"\n",
    "        seq_len, dim needed to pre-build sub-networks with dummy data\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.dim = dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.module = module\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Create sub-models\n",
    "        self.embedder = Embedder(hidden_dim, num_layers, module)\n",
    "        self.recovery = Recovery(hidden_dim, dim, num_layers, module)\n",
    "        self.generator = Generator(hidden_dim, num_layers, module)\n",
    "        self.supervisor = Supervisor(hidden_dim, num_layers, module)\n",
    "        self.discriminator = Discriminator(hidden_dim, num_layers, module)\n",
    "\n",
    "        # Build variables by passing dummy data once\n",
    "        self._build_submodules()\n",
    "\n",
    "        # # Create optimizers with low lr and gradient clipping\n",
    "        self.e_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0) #higher on e\n",
    "        self.g_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, clipnorm=1.0)\n",
    "        self.d_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, clipnorm=1.0)\n",
    "\n",
    "    def _build_submodules(self):\n",
    "        # dummy batch: shape (1, seq_len, dim)\n",
    "        x_dummy = tf.zeros([1, self.seq_len, self.dim], dtype=tf.float32)\n",
    "        z_dummy = tf.zeros([1, self.seq_len, self.dim], dtype=tf.float32)\n",
    "\n",
    "        # pass through each sub-network once\n",
    "        h = self.embedder(x_dummy)\n",
    "        _ = self.recovery(h)\n",
    "        e_hat = self.generator(z_dummy)\n",
    "        h_hat = self.supervisor(e_hat)\n",
    "        _ = self.discriminator(h_hat)\n",
    "\n",
    "    @tf.function\n",
    "    def train_embedder(self, x):\n",
    "        \"\"\"Phase 1: embedder/recovery MSE AE loss.\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            h = self.embedder(x)\n",
    "            x_tilde = self.recovery(h)\n",
    "            e_mse = tf.reduce_mean(tf.square(x - x_tilde))\n",
    "            e_loss = 10.0*tf.sqrt(e_mse)\n",
    "        grads = tape.gradient(e_loss, self.embedder.trainable_variables + self.recovery.trainable_variables)\n",
    "        self.e_optimizer.apply_gradients(zip(grads, self.embedder.trainable_variables + self.recovery.trainable_variables))\n",
    "        return e_loss\n",
    "\n",
    "    @tf.function\n",
    "    def train_supervised(self, x):\n",
    "        \"\"\"Phase 2: supervised next-step prediction in latent space.\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            h = self.embedder(x)\n",
    "            h_hat_sup = self.supervisor(h)\n",
    "            s_mse = tf.reduce_mean(tf.square(h[:,1:,:] - h_hat_sup[:,:-1,:]))\n",
    "        grads = tape.gradient(s_mse, self.generator.trainable_variables + self.supervisor.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_variables + self.supervisor.trainable_variables))\n",
    "        return s_mse\n",
    "\n",
    "    @tf.function\n",
    "    def train_joint(self, x):\n",
    "        \"\"\"\n",
    "        Phase 3: Joint training\n",
    "        2 generator updates, 1 discriminator update each step.\n",
    "        \"\"\"\n",
    "        # 1) Two generator updates\n",
    "        for _ in range(2):\n",
    "            with tf.GradientTape() as tape_g:\n",
    "                # Real\n",
    "                h = self.embedder(x)\n",
    "                x_tilde = self.recovery(h)\n",
    "\n",
    "                # Fake\n",
    "                z = tf.random.normal([tf.shape(x)[0], self.seq_len, self.dim], dtype=tf.float32)\n",
    "\n",
    "                e_hat = self.generator(z)\n",
    "                h_hat = self.supervisor(e_hat)\n",
    "\n",
    "                h_hat_sup = self.supervisor(h)\n",
    "                x_hat = self.recovery(h_hat)\n",
    "\n",
    "                y_fake = self.discriminator(h_hat)\n",
    "                y_fake_e = self.discriminator(e_hat)\n",
    "\n",
    "                # G losses\n",
    "                g_loss_u = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_fake, labels=tf.ones_like(y_fake)))\n",
    "                g_loss_u_e = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_fake_e, labels=tf.ones_like(y_fake_e)))\n",
    "\n",
    "                g_loss_s = tf.reduce_mean(tf.square(h[:,1:,:] - h_hat_sup[:,:-1,:]))\n",
    "\n",
    "                x_mean, x_var = tf.nn.moments(x, axes=[0,1])\n",
    "                x_hat_mean, x_hat_var = tf.nn.moments(x_hat, axes=[0,1])\n",
    "                g_loss_v1 = tf.reduce_mean(tf.abs(tf.sqrt(x_hat_var+1e-6) - tf.sqrt(x_var+1e-6)))\n",
    "                g_loss_v2 = tf.reduce_mean(tf.abs(x_hat_mean - x_mean))\n",
    "                g_loss_v = g_loss_v1 + g_loss_v2\n",
    "\n",
    "                g_loss = g_loss_u + self.gamma*g_loss_u_e + 100*tf.sqrt(g_loss_s) + 100*g_loss_v\n",
    "\n",
    "            grads_g = tape_g.gradient(g_loss, self.generator.trainable_variables + self.supervisor.trainable_variables)\n",
    "            self.g_optimizer.apply_gradients(zip(grads_g, self.generator.trainable_variables + self.supervisor.trainable_variables))\n",
    "\n",
    "            # Also update embedder a bit\n",
    "            with tf.GradientTape() as tape_e:\n",
    "                h2 = self.embedder(x)\n",
    "                x_tilde2 = self.recovery(h2)\n",
    "                e_mse = tf.reduce_mean(tf.square(x - x_tilde2))\n",
    "                e_loss_0 = 10.0*tf.sqrt(e_mse)\n",
    "                h_hat_sup2 = self.supervisor(h2)\n",
    "                e_s_loss = tf.reduce_mean(tf.square(h2[:,1:,:] - h_hat_sup2[:,:-1,:]))\n",
    "                e_loss = e_loss_0 + 0.1*e_s_loss\n",
    "\n",
    "            grads_e = tape_e.gradient(e_loss, self.embedder.trainable_variables + self.recovery.trainable_variables)\n",
    "            self.e_optimizer.apply_gradients(zip(grads_e, self.embedder.trainable_variables + self.recovery.trainable_variables))\n",
    "\n",
    "        # 2) Discriminator update\n",
    "        with tf.GradientTape() as tape_d:\n",
    "            h3 = self.embedder(x)\n",
    "            z2 = tf.random.uniform([tf.shape(x)[0], self.seq_len, self.dim], dtype=tf.float32)\n",
    "            e_hat2 = self.generator(z2)\n",
    "            h_hat2 = self.supervisor(e_hat2)\n",
    "\n",
    "            y_real = self.discriminator(h3)\n",
    "            y_fake2 = self.discriminator(h_hat2)\n",
    "            y_fake_e2 = self.discriminator(e_hat2)\n",
    "\n",
    "            d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_real, labels=tf.ones_like(y_real)))\n",
    "            d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_fake2, labels=tf.zeros_like(y_fake2)))\n",
    "            d_loss_fake_e = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_fake_e2, labels=tf.zeros_like(y_fake_e2)))\n",
    "            d_loss = d_loss_real + d_loss_fake + self.gamma*d_loss_fake_e\n",
    "\n",
    "        grads_d = tape_d.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        self.d_optimizer.apply_gradients(zip(grads_d, self.discriminator.trainable_variables))\n",
    "\n",
    "        return g_loss, d_loss, e_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gkANVfnM5Q0_"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 5) Metrics: Discriminative & Predictive\n",
    "###############################################################################\n",
    "\n",
    "def posthoc_discriminative_score(ori_data, gen_data, epochs=1000, batch_size=128):\n",
    "    \"\"\"\n",
    "    Train a simple GRU classifier to discriminate real vs. fake 1D time series.\n",
    "    Assumes input data is either shape (samples, timesteps) or (samples, timesteps, features).\n",
    "    If 2D, a singleton feature dimension is added.\n",
    "    \"\"\"\n",
    "    # Ensure data has 3 dimensions: (samples, timesteps, features)\n",
    "    if ori_data.ndim == 2:\n",
    "        ori_data = np.expand_dims(ori_data, -1)\n",
    "    if gen_data.ndim == 2:\n",
    "        gen_data = np.expand_dims(gen_data, -1)\n",
    "\n",
    "    n_ori = len(ori_data)\n",
    "    n_gen = len(gen_data)\n",
    "    x_all = np.concatenate([ori_data, gen_data], axis=0)\n",
    "    y_all = np.concatenate([np.ones(n_ori), np.zeros(n_gen)], axis=0)\n",
    "\n",
    "    # Shuffle data\n",
    "    idx = np.random.permutation(len(x_all))\n",
    "    x_all = x_all[idx]\n",
    "    y_all = y_all[idx]\n",
    "\n",
    "    # Split into train and test\n",
    "    split = int(0.8 * len(x_all))\n",
    "    x_train, x_test = x_all[:split], x_all[split:]\n",
    "    y_train, y_test = y_all[:split], y_all[split:]\n",
    "\n",
    "    timesteps = x_all.shape[1]\n",
    "    features = x_all.shape[2]\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=(timesteps, features)),\n",
    "        layers.GRU(units=features, activation='tanh'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    _, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "    # Return the absolute deviation of accuracy from random guessing (0.5)\n",
    "    return abs(acc - 0.5)\n",
    "\n",
    "def posthoc_predictive_score(ori_data, gen_data, epochs=1000, batch_size=128):\n",
    "    \"\"\"\n",
    "    Train a one-step-ahead predictor on the generated 1D time series data and\n",
    "    evaluate on original data. In a 1D setting, we construct input/target pairs\n",
    "    via time-shifting: for each sample, use all but the last time step as input (X)\n",
    "    and all but the first time step as target (Y).\n",
    "    \"\"\"\n",
    "    # Ensure data shape is (samples, timesteps, features)\n",
    "    if ori_data.ndim == 2:\n",
    "        ori_data = np.expand_dims(ori_data, -1)\n",
    "    if gen_data.ndim == 2:\n",
    "        gen_data = np.expand_dims(gen_data, -1)\n",
    "\n",
    "    def to_xy(data):\n",
    "        X_ = data[:, :-1, :]\n",
    "        Y_ = data[:, 1:, 0]\n",
    "        return X_, Y_\n",
    "\n",
    "    Xg, Yg = to_xy(gen_data)\n",
    "    Xo, Yo = to_xy(ori_data)\n",
    "\n",
    "    # Build a predictor model that outputs a prediction at every time step\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=(Xg.shape[1], Xg.shape[2])),\n",
    "        layers.GRU(64, return_sequences=True),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(Xg, Yg, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # Get predictions on the original (real) data\n",
    "    pred = model.predict(Xo, verbose=0)  # shape: (samples, timesteps, 1)\n",
    "\n",
    "    mae_list = []\n",
    "    for i in range(len(Yo)):\n",
    "        mae_list.append(mean_absolute_error(Yo[i], pred[i, :, 0]))\n",
    "    return np.mean(mae_list)\n",
    "\n",
    "def generate_synthetic_data(model, n_samples, batch_size):\n",
    "    \"\"\"\n",
    "    Generates synthetic data from the TimeGAN model in mini-batches.\n",
    "    \"\"\"\n",
    "    synthetic_data_batches = []\n",
    "    n_batches = int(np.ceil(n_samples / batch_size))\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        current_batch_size = min(batch_size, n_samples - i * batch_size)\n",
    "        # Generate a mini-batch\n",
    "        z = tf.random.normal([current_batch_size, model.seq_len, model.dim], dtype=tf.float32)\n",
    "        # Pass through generator, supervisor, and recovery\n",
    "        e_hat = model.generator(z)\n",
    "        h_hat = model.supervisor(e_hat)\n",
    "        x_hat = model.recovery(h_hat).numpy()\n",
    "        synthetic_data_batches.append(x_hat)\n",
    "\n",
    "    synthetic_data = np.concatenate(synthetic_data_batches, axis=0)\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BDn_aHvd6Lq6"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 6) Data Visualization\n",
    "###############################################################################\n",
    "def plot_reconstruction_comparison(x_real, x_recon):\n",
    "    \"\"\"\n",
    "    Plots real vs. reconstructed data.\n",
    "    x_real: shape (seq_len, dim)\n",
    "    x_recon: shape (seq_len, dim)\n",
    "    \"\"\"\n",
    "    seq_len, dim = x_real.shape\n",
    "\n",
    "    if dim == 1:\n",
    "        # If there's only one dimension, just plot a single series\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.plot(x_real[:, 0], label=\"Real\", color='blue')\n",
    "        plt.plot(x_recon[:, 0], label=\"Reconstructed\", color='red', linestyle='--')\n",
    "        plt.title(\"Reconstruction\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"Reconstruction.png\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Multiple dimensions: Plot each dimension in its own subplot\n",
    "        fig, axs = plt.subplots(dim, 1, figsize=(6, 2*dim))\n",
    "        for d in range(dim):\n",
    "            axs[d].plot(x_real[:, d], label=\"Real\", color='blue')\n",
    "            axs[d].plot(x_recon[:, d], label=\"Reconstructed\", color='red', linestyle='--')\n",
    "            axs[d].set_title(f\"Dimension {d}\")\n",
    "            axs[d].legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"Reconstruction.png\")\n",
    "        plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_joint_phase_loss(g_losses, d_losses, e_losses, filename=\"Joint_Loss_Curves.png\"):\n",
    "    \"\"\"\n",
    "    Plots the joint phase loss curves for G_loss, D_loss, and E_loss.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(g_losses, label=\"G_loss\")\n",
    "    plt.plot(d_losses, label=\"D_loss\")\n",
    "    plt.plot(e_losses, label=\"E_loss (Phase 3)\")\n",
    "    plt.xlabel(\"Training Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Joint Phase Loss Curves\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def visualize_samples(ori_data, gen_data, n_samples=5):\n",
    "    \"\"\"\n",
    "    Randomly picks n_samples from ori_data and gen_data and plots them in each dimension\n",
    "    \"\"\"\n",
    "    # Randomly choose real and fake data\n",
    "    idx_real = np.random.choice(len(ori_data), size=n_samples, replace=False)\n",
    "    idx_fake = np.random.choice(len(gen_data), size=n_samples, replace=False)\n",
    "\n",
    "    # Dimensions\n",
    "    n_dimensions = ori_data.shape[2]\n",
    "\n",
    "    plt.figure(figsize=(12, 4 * n_dimensions))\n",
    "\n",
    "    for dim in range(n_dimensions):\n",
    "        plt.subplot(n_dimensions, 1, dim + 1)\n",
    "\n",
    "        # Plot real data samples\n",
    "        for i in idx_real:\n",
    "            plt.plot(ori_data[i, :, dim], color='blue', alpha=0.5, label='Real' if dim == 0 and i == idx_real[0] else \"\")\n",
    "\n",
    "        # Plot fake data samples\n",
    "        for j in idx_fake:\n",
    "            plt.plot(gen_data[j, :, dim], color='red', alpha=0.5, label='Fake' if dim == 0 and j == idx_fake[0] else \"\")\n",
    "\n",
    "        plt.title(f\"Synthetic and Generated Samples (dim: {dim})\")\n",
    "        plt.xlabel(\"Time Step\")\n",
    "        plt.grid(True)\n",
    "\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    if handles and labels:\n",
    "        plt.figlegend(handles, labels, loc='upper center', ncol=2)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(\"Synthetic_and_Generated_Data.png\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_fft_distribution(data, sampling_rate=2000, name=\"\"):\n",
    "    \"\"\"\n",
    "    data: numpy array of shape (n_samples, timesteps, 1)\n",
    "    sampling_rate: number of samples per second, default=2000\n",
    "    \"\"\"\n",
    "    n_samples, timesteps, dims = data.shape\n",
    "    assert dims == 1, \"This function assumes a single-dimension signal.\"\n",
    "\n",
    "    data_2d = data.squeeze(axis=-1) # shape (n_samples, timesteps)\n",
    "\n",
    "    # Compute the FFT\n",
    "    fft_vals = np.fft.rfft(data_2d, axis=1)  # shape (n_samples, timesteps//2 + 1)\n",
    "    amplitude = np.abs(fft_vals) / timesteps\n",
    "    freqs = np.fft.rfftfreq(timesteps, d=1/sampling_rate)\n",
    "\n",
    "    mean_amplitude = amplitude.mean(axis=0)\n",
    "    std_amplitude = amplitude.std(axis=0)\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(freqs, mean_amplitude, label='Mean Amplitude')\n",
    "    plt.fill_between(\n",
    "        freqs,\n",
    "        mean_amplitude - std_amplitude,\n",
    "        mean_amplitude + std_amplitude,\n",
    "        alpha=0.3,\n",
    "        label='±1 std dev'\n",
    "    )\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title('FFT Spectrum with Mean ± std (Normal Distribution Envelope)')\n",
    "    plt.legend()\n",
    "    plt.xlim(25, 125) # TODO: Scale this with frequency\n",
    "    plt.savefig(f\"FFT_Spectrum{name}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UsvTrI8BWpBg"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 7) Synthetic Data Generation\n",
    "###############################################################################\n",
    "def generate_synthetic_data(model, n_samples, batch_size):\n",
    "    \"\"\"\n",
    "    Generates synthetic data from the TimeGAN model in mini-batches.\n",
    "    \"\"\"\n",
    "    synthetic_data_batches = []\n",
    "    n_batches = int(np.ceil(n_samples / batch_size))\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        current_batch_size = min(batch_size, n_samples - i * batch_size)\n",
    "        # Generate a mini-batch\n",
    "        z = tf.random.normal([current_batch_size, model.seq_len, model.dim], dtype=tf.float32)\n",
    "        # Pass through generator, supervisor, and recovery\n",
    "        e_hat = model.generator(z)\n",
    "        h_hat = model.supervisor(e_hat)\n",
    "        x_hat = model.recovery(h_hat).numpy()\n",
    "        synthetic_data_batches.append(x_hat)\n",
    "\n",
    "    synthetic_data = np.concatenate(synthetic_data_batches, axis=0)\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pa4mpv7Z5QoU"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 8) The main run function\n",
    "###############################################################################\n",
    "\n",
    "def run_timegan_tf2_sine(\n",
    "    n_samples=10000,\n",
    "    seq_len=24,\n",
    "    base_freq=5,\n",
    "    sampling_rate=100,\n",
    "    dim=5,\n",
    "    hidden_dim=24,\n",
    "    num_layers=3,\n",
    "    module='gru',\n",
    "    gamma=1.0,\n",
    "    iterations=2000,\n",
    "    batch_size=128,\n",
    "    metric_iterations=3,\n",
    "    patience=20\n",
    "\n",
    "):\n",
    "    ############################################################################\n",
    "    # Phase 0: Data Generation and Model declaration\n",
    "    ############################################################################\n",
    "\n",
    "    # Generate Synthetic Sine Data\n",
    "    print(\"Generating Sine Data ...\")\n",
    "    ori_data = sine_data_generation(\n",
    "        n_samples=n_samples,\n",
    "        seq_len=seq_len,\n",
    "        sampling_rate=sampling_rate,\n",
    "        base_freq=base_freq,\n",
    "        dim=dim\n",
    "    )\n",
    "\n",
    "    # n_timesteps = seq_len * sampling_rate\n",
    "    n_timesteps = ori_data.shape[1]\n",
    "\n",
    "    print(f\"Original data shape: {ori_data.shape}\")\n",
    "\n",
    "\n",
    "    t = np.linspace(0, seq_len, n_timesteps, endpoint=False)\n",
    "    plt.plot(t, ori_data[0], label=\"Sine wave\")\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(f\"{base_freq}Hz sine wave, {seq_len} seconds @ {sampling_rate} Hz sampling\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"Synthetic_Data.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Scale data (min-max)\n",
    "    ori_data_scaled, min_val, max_val = min_max_scale(ori_data)\n",
    "\n",
    "    # Build TimeGAN\n",
    "    timegan_model = TimeGAN(\n",
    "        seq_len=n_timesteps,   # <-- temporary fix. new data shape: (n_samples, n_timesteps, dim)\n",
    "        dim=dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        module=module,\n",
    "        gamma=gamma\n",
    "    )\n",
    "\n",
    "    # For Early Stopping\n",
    "    best_e_loss = float('inf')\n",
    "    no_improve_steps_e = 0\n",
    "    best_g_loss = float('inf')\n",
    "    no_improve_steps_g = 0\n",
    "\n",
    "    ############################################################################\n",
    "    # Phase 1: Embedding network training\n",
    "    ############################################################################\n",
    "    print(\"Phase 1: Embedding network training ...\")\n",
    "\n",
    "    # Training Loop\n",
    "    for step in range(iterations):\n",
    "        x_mb = batch_generator(ori_data_scaled, batch_size)\n",
    "        e_loss = timegan_model.train_embedder(x_mb)\n",
    "\n",
    "\n",
    "     # Early stopping logic for Phase 1\n",
    "        current_loss = e_loss.numpy()\n",
    "        if current_loss < best_e_loss:\n",
    "            best_e_loss = current_loss\n",
    "            no_improve_steps_e = 0\n",
    "        else:\n",
    "            no_improve_steps_e += 1\n",
    "\n",
    "        # Print embedding loss every 50 steps\n",
    "        if (step+1) % 50 == 0:\n",
    "            print(f\" Step {step+1}/{iterations} | E_loss: {current_loss:.4f}\")\n",
    "\n",
    "        if no_improve_steps_e >= patience:\n",
    "            print(f\"Early stopping triggered in Phase 1 at step {step+1} | E_loss: {current_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    ############################################################################\n",
    "    # Phase 1.1: Plotting Reconstruction Error\n",
    "    ############################################################################\n",
    "\n",
    "    # Suppose x_mb is a small batch of shape (batch_size, seq_len, dim)\n",
    "    x_mb = ori_data_scaled[:2]\n",
    "\n",
    "    # Get hidden representation\n",
    "    h_mb = timegan_model.embedder(x_mb)\n",
    "\n",
    "    # Reconstruct\n",
    "    x_tilde_mb = timegan_model.recovery(h_mb)\n",
    "\n",
    "    recon_error = tf.reduce_mean(tf.square(x_mb - x_tilde_mb)).numpy()\n",
    "    print(\"Reconstruction error after Phase 1:\", recon_error)\n",
    "\n",
    "    # Plot\n",
    "    plot_reconstruction_comparison(x_mb[0], x_tilde_mb[0])\n",
    "\n",
    "\n",
    "    ############################################################################\n",
    "    # Phase 2: Supervised loss training\n",
    "    ############################################################################\n",
    "\n",
    "    print(\"Phase 2: Supervised loss training ...\")\n",
    "\n",
    "    best_s_loss = float('inf')\n",
    "    no_improve_steps_s = 0\n",
    "\n",
    "    for step in range(iterations):\n",
    "        x_mb = batch_generator(ori_data_scaled, batch_size)\n",
    "        s_loss = timegan_model.train_supervised(x_mb)\n",
    "\n",
    "        # Early stopping logic for Phase 2\n",
    "        current_loss = s_loss.numpy()\n",
    "        if current_loss < best_s_loss:\n",
    "            best_s_loss = current_loss\n",
    "            no_improve_steps_s = 0\n",
    "        else:\n",
    "            no_improve_steps_s += 1\n",
    "\n",
    "        if (step+1) % 50 == 0:\n",
    "            print(f\" Step {step+1}/{iterations} | S_loss: {current_loss:.4f}\")\n",
    "\n",
    "        if no_improve_steps_s >= patience:\n",
    "            print(f\"Early stopping triggered in Phase 2 at step {step+1} | S_loss: {current_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    ############################################################################\n",
    "    # Phase 2.1: Print Supervisor Error\n",
    "    ############################################################################\n",
    "\n",
    "    h_mb = timegan_model.embedder(ori_data_scaled[:2])\n",
    "    h_hat_sup = timegan_model.supervisor(h_mb)\n",
    "    sup_error = tf.reduce_mean(tf.square(h_mb[:,1:,:] - h_hat_sup[:,:-1,:])).numpy()\n",
    "    print(\"Supervisor next-step error after Phase 2:\", sup_error)\n",
    "\n",
    "    ############################################################################\n",
    "    # Phase 3: Joint training\n",
    "    ############################################################################\n",
    "    print(\"Phase 3: Joint training (2 G updates, 1 D update per iteration) ...\")\n",
    "\n",
    "    # For loss plotting\n",
    "    g_losses, d_losses, e_losses = [], [], []\n",
    "\n",
    "    # Training Loop\n",
    "    for step in range(iterations):\n",
    "        x_mb = batch_generator(ori_data_scaled, batch_size)\n",
    "        g_loss, d_loss, e_loss_val = timegan_model.train_joint(x_mb)\n",
    "\n",
    "        # Append losses\n",
    "        g_losses.append(g_loss.numpy())\n",
    "        d_losses.append(d_loss.numpy())\n",
    "        e_losses.append(e_loss_val.numpy())\n",
    "\n",
    "        # Early stopping logic for Phase 3 using g_loss\n",
    "        current_g_loss = g_loss.numpy()\n",
    "        if current_g_loss < best_g_loss:\n",
    "            best_g_loss = current_g_loss\n",
    "            no_improve_steps_g = 0\n",
    "        else:\n",
    "            no_improve_steps_g += 1\n",
    "\n",
    "        if (step+1) % 50 == 0:\n",
    "            print(f\" Step {step+1}/{iterations} | G_loss: {g_loss.numpy():.4f}\"\n",
    "                  f\" | D_loss: {d_loss.numpy():.4f} | E_loss: {e_loss_val.numpy():.4f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if no_improve_steps_g >= patience:\n",
    "            print(f\"Early stopping triggered in Phase 3 at step {step+1} | G_loss: {g_loss.numpy():.4f}\"\n",
    "                  f\" | D_loss: {d_loss.numpy():.4f} | E_loss: {e_loss_val.numpy():.4f}\")\n",
    "            break\n",
    "\n",
    "    ############################################################################\n",
    "    # Phase 3.1: Plotting Joint Training Losses\n",
    "    ############################################################################\n",
    "\n",
    "    plot_joint_phase_loss(g_losses, d_losses, e_losses)\n",
    "\n",
    "    ############################################################################\n",
    "    # Phase 4: Evaluating Model\n",
    "    ############################################################################\n",
    "\n",
    "    print(\"Generating synthetic data ...\")\n",
    "    n_samples = len(ori_data_scaled)\n",
    "\n",
    "    # Use a mini-batch size that fits in memory\n",
    "    x_hat = generate_synthetic_data(timegan_model, n_samples, batch_size)\n",
    "\n",
    "    # Denormalize\n",
    "    x_hat_rescaled = x_hat*(max_val+1e-7) + min_val\n",
    "\n",
    "    # Plot real and fake mean/std\n",
    "    real_mean = np.mean(ori_data, axis=(0,1))\n",
    "    real_std = np.std(ori_data, axis=(0,1))\n",
    "\n",
    "    fake_mean = np.mean(x_hat_rescaled, axis=(0,1))\n",
    "    fake_std = np.std(x_hat_rescaled, axis=(0,1))\n",
    "\n",
    "    print(\"Real mean:\", real_mean)\n",
    "    print(\"Fake mean:\", fake_mean)\n",
    "    print(\"Real std:\", real_std)\n",
    "    print(\"Fake std:\", fake_std)\n",
    "\n",
    "    print(\"Computing metrics ...\")\n",
    "    disc_scores = []\n",
    "    pred_scores = []\n",
    "    for _ in range(metric_iterations):\n",
    "        disc = posthoc_discriminative_score(ori_data, x_hat_rescaled, epochs=5)\n",
    "        pred = posthoc_predictive_score(ori_data, x_hat_rescaled, epochs=5)\n",
    "        disc_scores.append(disc)\n",
    "        pred_scores.append(pred)\n",
    "    disc_mean = np.mean(disc_scores)\n",
    "    pred_mean = np.mean(pred_scores)\n",
    "\n",
    "    print(f\"Discriminative Score: {disc_mean:.4f}\")\n",
    "    print(f\"Predictive Score   : {pred_mean:.4f}\")\n",
    "\n",
    "    return ori_data, x_hat_rescaled, disc_mean, pred_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ryGE2avVUIVl",
    "outputId": "1f322f0e-acea-486d-8dff-4b9594f5f69f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Running TimeGAN TF2 with sub-network pre-building ...\n",
      "Generating Sine Data ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning TimeGAN TF2 with sub-network pre-building ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     ori_data, gen_data, disc_score, pred_score \u001b[38;5;241m=\u001b[39m \u001b[43mrun_timegan_tf2_sine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlstm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlotting FFT distributions ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m     plot_fft_distribution(gen_data[:,:,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m], sampling_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e4\u001b[39m) \u001b[38;5;66;03m# Works in multiple dimensions by only using the first dim\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 27\u001b[0m, in \u001b[0;36mrun_timegan_tf2_sine\u001b[1;34m(n_samples, seq_len, base_freq, sampling_rate, dim, hidden_dim, num_layers, module, gamma, iterations, batch_size, metric_iterations, patience)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_timegan_tf2_sine\u001b[39m(\n\u001b[0;32m      6\u001b[0m     n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m,\n\u001b[0;32m      7\u001b[0m     seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Generate Synthetic Sine Data\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating Sine Data ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m     ori_data \u001b[38;5;241m=\u001b[39m \u001b[43msine_data_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# n_timesteps = seq_len * sampling_rate\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     n_timesteps \u001b[38;5;241m=\u001b[39m ori_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[4], line 35\u001b[0m, in \u001b[0;36msine_data_generation\u001b[1;34m(n_samples, seq_len, dim, sampling_rate, base_freq)\u001b[0m\n\u001b[0;32m     33\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msin(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m*\u001b[39m f \u001b[38;5;241m*\u001b[39m t \u001b[38;5;241m+\u001b[39m phase)\n\u001b[0;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m---> 35\u001b[0m     seq\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[0;32m     36\u001b[0m seq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(np\u001b[38;5;241m.\u001b[39marray(seq))\n\u001b[0;32m     37\u001b[0m data\u001b[38;5;241m.\u001b[39mappend(seq)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 9) Runs the TimeGAN\n",
    "###############################################################################\n",
    "\n",
    "print(tf.executing_eagerly())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running TimeGAN TF2 with sub-network pre-building ...\")\n",
    "    ori_data, gen_data, disc_score, pred_score = run_timegan_tf2_sine(\n",
    "        n_samples=500,\n",
    "        seq_len=.1,\n",
    "        base_freq=1e3,\n",
    "        sampling_rate=1e6,\n",
    "        dim=1,\n",
    "        hidden_dim=32,\n",
    "        num_layers=3,\n",
    "        module='lstm',\n",
    "        gamma=1.0,\n",
    "        iterations=300,\n",
    "        batch_size=32,\n",
    "        metric_iterations=3,\n",
    "        patience=32\n",
    "    )\n",
    "\n",
    "    print(\"Plotting FFT distributions ...\")\n",
    "    plot_fft_distribution(gen_data[:,:,0:1], sampling_rate=1e4) # Works in multiple dimensions by only using the first dim\n",
    "    plot_fft_distribution(ori_data[:,:,0:1], sampling_rate=1e4)\n",
    "\n",
    "\n",
    "    print(\"Visualizing data samples ...\")\n",
    "    for i in range(4):\n",
    "      plt.plot(gen_data[i])\n",
    "      plt.savefig(f\"gen_data{i}.png\")\n",
    "      plt.show()\n",
    "\n",
    "    visualize_samples(ori_data, gen_data, n_samples=10)\n",
    "\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
